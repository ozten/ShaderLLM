# TODO

## Setup Basic Supervised Fine-Tuning (SFT) Evaluation Metrics

I setup Weights & Biases.

Next steps: https://aistudio.google.com/prompts/1OHq4FLra7YIdginsCz1WhuPDya5x02Gy

Monitor Training: Watch the loss curves carefully. Ensure the model is learning something.

Evaluate Qualitatively: Look at the generated shaders for validation images. Do they make any sense relative to the image, even if imperfect?

Evaluate larger 1.5B Qwen against 0.5.

## Create more training and test data

Write a script that takes a templated shader design and explores all permutations
and writes the shaders out to disk.

## Improved Prompts

glslViewer code is different than the vanilla glsl code generated by coding LLMs.
We need a few examples in the prompt.

## Setup RL 

Evaluate the shader. Score the shader compilation and image similarity of the output.

## Evolve Training loop to be SFT and RL

Do 4 SFT steps and 1 RL step (something like this).

## Improving our architecture

### Using Patch Features (More Detailed Visual Input)

See: experiments/VisionAugmentedLLM_forward_patch.py

This is the most common and generally recommended improvement.

Extract Patch Features: Instead of get_image_features(), access the output sequence from the CLIP vision transformer's final layer before pooling. This gives you a sequence of embeddings, where each embedding corresponds to a patch of the input image (e.g., for ViT-B/32 on a 224x224 image, you get (224/32)^2 = 7x7 = 49 patch embeddings, plus potentially a class token).

Project Patch Features: Project each of these patch embeddings to the LLM's hidden dimension using your vision_projection layer (it will now operate on a sequence).

Prepend Feature Sequence: Prepend this sequence of projected visual embeddings to the text embeddings.

Adjust Masking/Labels: Update the attention_mask to cover these new visual tokens and shift the labels accordingly (assigning -100 to all visual tokens).

### LORA

LLM Fine-tuning Strategy: Instead of full fine-tuning, consider using Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA (Low-Rank Adaptation). Apply LoRA to the LLM's attention blocks (query, key, value projections). This often works well for adapting LLMs to new tasks/modalities without catastrophic forgetting or excessive resource usage. You'd train the LoRA weights and the vision_projection layer (and potentially the Perceiver if using Approach 2).

### Single Feature Vector

Current: You're using vision_model.get_image_features(). For CLIP, this typically returns a single embedding vector (often the embedding corresponding to the [CLS] token after the Vision Transformer) representing the global semantics of the image.

Problem: Generating detailed shader code often requires understanding spatial relationships, textures, patterns, and local details. A single global vector might be too coarse and lose this crucial information. It tells the LLM what the image is about (e.g., "a tree") but not how it looks in detail across the canvas.

Symptom: The LLM knows it should be getting an image input (based on the prompt structure learned during training) but the single vector isn't informative enough, so it falls back to its base behavior, possibly repeating parts of the training prompts 
or asking for clarification.

### Simple Prepending

Current: You project the single vision vector and prepend it to the text token embeddings.

Problem: While prepending is a valid technique, the LLM needs to learn how to attend to and utilize this special visual token effectively. With only one visual token containing compressed global information, the signal might be too weak or get lost among the many text tokens, especially during the complex reasoning needed for code generation. The projection layer (vision_projection) is the only bridge, and it might not have learned a sufficiently meaningful transformation during training.

## Explore other architectures

### Textual Description

A simplier architecture is to have CLIP describe our target image
and feed that description into a coding LLM.

### Cross Attention

train_with_vision_cross_attention_model.py <- sketch, not tested

## mps doesn't work

simple_text_only_run.py demonstrates running on MPS with the LLM.

test_mps_generate_embeds.py demonstrates that our current vision + llm architecture doesn't work on mps,
so we have to do inference on CPU.